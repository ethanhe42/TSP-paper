\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{An Empirical Analysis of Approximation Algorithms for the Euclidean Traveling Salesman Problem
}

\author{Yihui He\\
Xi'an Jiaotong University\\
Xi'an, China\\
{\tt\small heyihui@stu.xjtu.edu.cn}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Ming Xiang\\
Xi'an Jiaotong University\\
Xi'an China\\
{\tt\small mxiang@mail.xjtu.edu.cn}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
With applications to many disciplines, the traveling salesman problem (TSP) is a classical computer science optimization problem with applications to industrial engineering, theoretical computer science, bioinformatics, and several other disciplines \cite{introduction}. In recent years, 
there have been a plethora of novel approaches for approximate solutions ranging from simplistic greedy to cooperative distributed algorithms derived from artificial intelligence. 
In this paper,
we perform an evaluation and analysis of cornerstone algorithms for the Euclidean TSP. We evaluate greedy, 2-opt, and genetic algorithms. We use several datasets as input for the algorithms including a small dataset, a medium-sized dataset representing
cities in the United States, and a synthetic dataset consisting of 200 cities to test algorithm scalability. We discover that
the greedy and 2-opt algorithms efficiently calculate solutions for smaller datasets. Genetic algorithm has the best
performance for optimality for medium
to large datasets, but generally have longer runtime. 
Our implementations is public available \footnote{https://github.com/yihui-he/TSP}.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Known to be NP-hard, the traveling salesman problem (TSP) was first formulated in 1930 and is one of the most studied optimization problems to date \cite{hoffman1986traveling}. The problem is as follows: given a list of cities and a distance between each pair of cities, find the shortest possible path that visits every city exactly once and returns to the starting city. The TSP has broad applications including: shortest-path
for lasers to sculpt microprocessors and delivery logistics for mail services, to name a few.

The TSP is an area of active research. In fact, several
variants have been derived from the original TSP. In this
paper, we focus on the Euclidean TSP. In the Euclidean TSP, the vertices correspond to points in a $d$-dimensional
space, and the cost function is the Euclidean distance. That is, the Euclidean distance between two cities $x = (x_1, x_2, ..., x_d)
, y = (y_1, y_2, ..., y_d)$ is:
\begin{equation}
	(\sum_{i=0}^{d}(x_i-y_i)^2)^{1/2}
\end{equation}
This simplification allows us to survey several cornerstone algorithms without introducing complex scenarios. The remainder of this paper is organized
as follows. In Section~\ref{sec:background}, we briefly review the first solutions and survey variants to the TSP. We
describe the algorithms used in our experiment in Section~\ref{sec:algo}. A description of
the benchmark datasets and results of the experiment are
detailed in Section~\ref{sec:exp}, and explains the
findings and compares the performance of the algorithms.
We then conclude and describe future work in Section~\ref{sec:conc}.

\section{Background}\label{sec:background}
An example TSP is illustrated in Figure~\ref{fig:egtsp}. The input is a collection of cities in the two dimensional space. This input can be represented as a distance matrix for each pair of cities or as a list of points
denoting the coordinate of each city. In the latter method,
distances are calculated using Euclidean geometry. A non-optimal tour is shown in sub-figure (b). Although not shown in the figure, each edge will have some non-negative edge
weight denoting the distance between two nodes or cities.
Due to the computational complexity of the TSP, it may be
necessary to approximate the optimal solution. The optimal
tour is shown in sub-figure (c). For small graphs, it may be
possible to perform an exhaustive search to obtain the optimal solution. However, as the number of cities increases, so
does the solutions space, problem complexity, and running
time.

\begin{figure}
\centering
\includegraphics[width=.7\linewidth]{egtsp}
\caption{}
\label{fig:egtsp}
\end{figure}
If $n$ is the number of cities. The number of possible edges is $\sum_{i=0}^{n-1}i$. The number of possible tours is $(n - 1)!/2$. since the same tour, with start point
X and Y appears twice: once with X as the start node and
once with Y as the start node.

The TSP was first formulated in the 1930s by Karl Menger in Vienna and Harvard. 
By the mid-1950s, solutions for TSP began to appear. 
The first solution was published by Dantzig, Fulkerson, and Johnson using a dataset of 49 cities. 
%The progression of solutions is shown in Table~\ref{fig:hist}. 
In 1972, Richard M. Karp proved that the Hamiltonian cycle problem was NP-Complete, 
which proves that the TSP is NP-Hard.
%\begin{table*}[]
%	\centering
%\caption{history of solutions for TSP \cite{haqueempirical}}
%\label{fig:hist}
%	\begin{tabular}{l|l|l}
%		\hline
%		Year & Research Team                                    & Cities \\ \hline
%		1954 & G. Dantzig, R. Fulkerson, and S. Johnson         & 49     \\ \hline
%		1971 & M. Held and R.M. Karp                            & 64     \\ \hline
%		1975 & P.M. Camerini, L. Fratta, and F. Maffioli        & 67     \\ \hline
%		1977 & M. Grotschel                                     & 120    \\ \hline
%		1980 & H. Crowder and M.W. Padberg                      & 318    \\ \hline
%		1987 & M. Padberg and G. Rinaldi                        & 532    \\ \hline
%		1987 & M. Grotschel and O. Holland                      & 666    \\ \hline
%		1987 & M. Padberg and G. Rinaldi                        & 2,392  \\ \hline
%		1994 & D. Applegate, R. Bixby, V. Chavatal, and W. Cook & 7,397  \\ \hline
%		1998 & D. Applegate, R. Bixby, V. Chavatal, and W. Cook & 13,509 \\ \hline
%		2004 & D. Applegate, R. Bixby, V. Chavatal, and W. Cook & 24,978 \\ \hline
%		2006 & D. Applegate, R. Bixby, V. Chavatal, and W. Cook & 85,900\\ \hline
%	\end{tabular}
%\end{table*}


In modern day, the TSP has a variety of applications to numerous fields. Examples among
these applications include genome sequencing, air traffic control, supplying manufacturing lines, and optimization.

%\subsection{TSP Variants}
%Several variants of the original TSP exist. Some of these
%variants differ in their representation of cities and some differ
%by the constraints placed on city distances. We describe the
%Euclidean, Graphic, and Asymmetric TSP variants in this
%section.
%\subsubsection{Euclidean TSP}
%In the Euclidean TSP, the input is given as a list of coordinates describing the location of each city in $R^2$ \cite{arora1996polynomial}. It is
%possible to extend this into higher dimensions as well. The
%alternative to giving a list of city coordinates is to give a
%distance/adjacency matrix describing the distances between
%each city pair. It is possible to derive the distance matrix given the city coordinates using Euclidean geometry.
%However, given the distance matrix, inferring the coordinates is not as straightforward and requires computation of
%a Gram matrix and application of several matrix decomposition methods.
%
%\subsubsection{Graphic TSP}
%In recent years, the graphic TSP has attracted attention
%from the research community. The graphic TSP problem
%asks for the shortest tour that visits each vertex at least
%once. The current best approximation algorithm returns a
%solution within $13/9$ of the optimal \cite{mucha2014frac}. For the
%past 30 years, Christofides had been the leading algorithm
%with a 1.5 approximation. Although the 13/9 approximation
%is for a TSP variant, it is an important milestone for TSP
%approximations nonetheless.
%
%\subsubsection{Asymmetric TSP}
%All TSP variants described thus far have assumed an undirected graph, resulting in cost(A, B) = cost(B, A). The
%asymmetric TSP introduces directed edges. As a consequence, algorithms with assumptions about reflexive distances may fail when cost(A, B) 6 = cost(B, A). Real world
%applications of this includes roads and highways – specifically the case of one-way streets, detours, and alternate
%routes. Additionally when modeling vehicle energy usage,
%the geographical topology makes traveling uphill cheaper
%than downhill despite traveling between the same two points.



\section{Algorithms}\label{sec:algo}
We now move to a discussion of the algorithms used in our evaluation. 
First, we describe an upper bound for TSP in Section~\ref{sec:rand}.
The traditional greedy and 2-opt approaches are discussed in Section~\ref{sec:greedy} and Section~\ref{sec:2opt}. 
We finally discuss the genetic algorithm in Section~\ref{sec:genetic}.

\subsection{Random Path}\label{sec:rand}
Finding the worst case of TSP is as hard as the best one.
So we uniformly generate a random path for all available edges, 
and use this as a upper bound of optimal path benchmark for all other algorithms.

\subsection{Greedy Algorithm}\label{sec:greedy}
The greedy heuristic is based on Kruskal’s algorithm to
give an approximate solution to the TSP \cite{kim1998comparison}. The algorithm
forms a tour of the shortest route and can be constructed if and only if:
The edges of the tour must not form a cycle unless
the selected number of edges is equal to the number of
vertices in the graph.
The selected edge (before being appended to the tour)
does not increase the degree of any node to be more
than 2.
The algorithm begins by sorting all edges from least weight
to most heavily weighted. After the edges are sorted, the
least heavily-weighted edge is selected and it is added to the tour if it does not violate the above conditions. The algorithm continues by selecting the next least-cost edge and adding it to the tour. This process is repeated until all vertices can be reached by the tour. The result is a minimum
spanning tree and is a solution for the TSP. The runtime for
the greedy algorithm is $O(n^2log(n))$ and generally returns a solution within 15-20\% of the Held-Karp lower bound \cite{rosenkrantz1977analysis}.

\subsection{2opt Algorithm}\label{sec:2opt}
In optimization, 2-opt is a simple local search algorithm first proposed by Croes in 1958 for solving the TSP \cite{croes1958method}.
The main idea behind it is to take a route that crosses over itself and reorder it so that it does not.

A complete 2-opt local search will compare every possible valid combination of the swapping mechanism. This technique can be applied to the travelling salesman problem as well as many related problems. These include the vehicle routing problem (VRP) as well as the capacitated VRP, which require minor modification of the algorithm.

This is the mechanism by which the 2-opt swap manipulates a given route:
\begin{enumerate}
\item take route[1] to route[i-1] and add them in order to new route
\item take route[i] to route[k] and add them in reverse order to new route
\item take route[k+1] to end and add them in order to new route
\item return new route
\end{enumerate}
 
%This pseudo-code of the complete 2-opt algorithm is shown below.
%
%\begin{lstlisting}
%repeat until no improvement is made {
%start_again:
%best_distance = TotalDist(existing_route)
%for (i=0;i<N eligible nodes-1;i++) {
% for (k=i+1;k<N eligible nodes;k++) {
%  new_route = 2optSwap(existing_route,i,k)
%  new_distance = TotalDist(new_route)
%  if (new_distance < best_distance) {
%	existing_route = new_route
%	goto start_again
%       }
%   }
%}}
%\end{lstlisting}

\subsection{Genetic Algorithm}\label{sec:genetic}
Genetic algorithms (GA) are search heuristics that at-
tempt to mimic natural selection for many problems in optimization and artificial intelligence \cite{grefenstette1985genetic}. In a genetic algorithm, a population of candidate solutions is evolved over
time towards better solutions. These evolutions generally
occur through mutations, randomization, and recombination. We define a fitness function to differentiate between
better and worse solutions. Solutions, or individuals, with
higher fitness scores are more likely to survive over time.
The final solution is found if the population converges to a solution within some threshold. However, great care must
be taken to avoid being trapped at local optima.

We will now apply a genetic algorithm to the TSP \cite{bryant2000genetic}. We define a fitness function F as the
length of the tour. Supposed we have an ordering of the
cities $A = {x_1 , x_2 , ..., x_n }$ where n is the number of cities.
The fitness score for the TSP becomes the cost of the tour
$d(x, y)$ denote the distance from x to y.
\begin{equation}
	F(A) =\sum_{i=0}^{n-1} d(x_i , x_{i+1}) + d(x_n , x_0 )
\end{equation}
The genetic algorithm begins with an initial, $P_0$ , random
population of candidate solutions. That is, we have a set of
paths that may or may not be good solutions. We then move
forward one time step. During this time step, we perform a
set of probabilistic and statistical methods to select, mutate,
and produce an offspring population, $P_1$ , with traits similar
to those of the best individuals (with the highest fitness)
from $P_0$ . We then repeat this process until our population becomes homogeneous.

The running time of genetic algorithms is variable and dependent on the problem and heuristics used. However, for
each individual in the population, we require $O(n)$ space for
storage of the path. For genetic crossover, the space requirement remains $O(n)$. The best genetic algorithms can find solutions within 2\% of the optimal tour for certain graphs \cite{homaifar1992schema}.

\section{Experiment}\label{sec:exp}
We benchmark our algorithms using publicly available
datasets. 
Additionally, to test the scalability of the algorithms, we generated a synthetic dataset consisting of 200 cities. 
In all dataset names, the numeric digits represent
the number of cities in the dataset. 
The datasets are as follows: P15, ATT48, and R200.
All datasets except R200 can be found online \cite{data0,data1}. The
ATT48 and SGB128 datasets represent real-data consisting
of locations of cities in the United States. 
A visual representation of the ATT48 dataset in the 2D plane is shown
in Figure~\ref{fig:00001}
\begin{figure}
\centering
\includegraphics[width=\linewidth]{../TSP_Animation/00001.png}
\caption{the ATT48 dataset in the 2D plane. (the United States)}
\label{fig:00001}
\end{figure}

Not all datasets have a known optimal tour. When this is the case, we use random path algorithm to infer a upper bound of the optimal tour. 

\subsection{Random Dataset}
The R200 dataset was generated by plotting 200 random, 
uniformly distributed points $(x, y)$, in $R^2$ with $(x, y) \in
[0, 4000]$. As a result, all
distances satisfy the triangle inequality and this dataset can
be classified as a Euclidean TSP dataset. The running time
for creating the dataset is $O(n)$. 
The output is a list of all cities represented as $(x,y)$ points.

\subsection{Comparison}
As we can see in Figure~\ref{fig:tsp}, the greedy is the most efficient. In Figure~\ref{fig:length},
we can see that most algorithms return a solution similar with the optimal for small datasets and become worse for larger datasets.
\begin{figure}
\centering
\includegraphics[width=\linewidth]{../TSP_Animation/time.pdf}
\caption{runtime comparison, the y axis is in log scale}
\label{fig:tsp}
\end{figure}

In terms of running time (Figure~\ref{fig:tsp}), the best algorithm is greedy algorithm. However, in terms of optimal tour length of solution, the best algorithm is GA. This is in line with our expectations and alludes to the fact that different heuristics
are better suited for different situations.
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{../TSP_Animation/length.pdf}
	\caption{tour length comparison, the y axis is in log scale, divided by random tour length}
	\label{fig:length}
\end{figure}

As shown in Figure~\ref{fig:length}, genetic algorithm performs fairly consistently, in comparison to the 2-opt and greedy algorithms, across all datasets. Highlighted in
Figure~\ref{fig:tsp}, the running time of genetic is almost linear. This suggests that for larger
datasets, if running time is a concern, then the genetic
algorithm should be used. Figure~\ref{fig:length} further demonstrates
that genetic algorithm maintains a smaller percent above
optimal than the other algorithms. From this, we can see
that genetic algorithm has high accuracy and better complexity than other heuristics, especially for larger datasets. Surprisingly, genetic algorithm got the optimal solution for att48 dataset, shown in Figure~\ref{fig:saatt}.
\begin{figure}
\centering
\includegraphics[width=1\linewidth]{../TSP_Animation/saatt.png}
\caption{Solution generated by genetic algorithm for att48 dataset}
\label{fig:saatt}
\end{figure}


\section{Conclusion}\label{sec:conc}
Most of our algorithms attempt to solve the TSP in a
linear fashion. Originating from artificial intelligence, the
genetic algorithm is very different compared to greedy, and 2-opt. Literature suggests that the
best algorithms focus on iteration and convergence to find
optimal tours -- something genetic algorithms attempt to
achieve. For example, the Large Step Markov Chain \cite{hong1997improved}
relies on Markov chains to find convergence of many paths
to form a global optimum and several papers cite Markov
Chains as the best known solution to TSP. Recent studies include using adaptive Markov Chain Monte Carlo algorithms \cite{qiu2008adaptive}. Many of these extend the Metropolis algorithm \cite{homaifar1992schema}, 
a simulated annealing algorithm which attempts to mimic randomness with particles as the temperature varies. This further supports our conclusion that algorithms inspired from
artificial intelligence perform well for finding solutions for the TSP. 
However, these may not be suitable when a guarantee is required.

In this paper, we surveyed several key cornerstone approaches to the TSP. We selected
four well-known algorithms and tested their performance on
a variety of public datasets. Our results suggest that genetic algorithms (and other approaches from artificial intelligence) are able to find a near-optimal solution.

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
